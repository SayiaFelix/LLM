{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a79da2-2641-4ec3-9b15-f8e2ad5f9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful technical tutor/assistant who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628041f0-be85-4fb5-b816-0b131eac5418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Explanation of the Code**\n",
       "\n",
       "The given code is a Python expression that uses several advanced features, including generators, dictionary lookups, and list comprehensions. Let's break it down step by step:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "*   `books`: This assumes that `books` is an iterable collection of dictionaries (e.g., a list or another generator).\n",
       "*   `{}`: This is a dictionary comprehension, which creates a new dictionary containing the values generated by the expression inside it.\n",
       "*   `for book in books if book.get(\"author\")`: This part filters out any items from `books` that don't have an \"author\" key. It's equivalent to writing a traditional for loop like this:\n",
       "\n",
       "    ```python\n",
       "filtered_books = []\n",
       "for book in books:\n",
       "    if book.get(\"author\"):\n",
       "        filtered_books.append(book)\n",
       "```\n",
       "\n",
       "    But the dictionary comprehension is more concise and Pythonic.\n",
       "*   `book.get(\"author\")`: For each book that passes the filter, this expression returns its author. If \"author\" doesn't exist as a key, it defaults to an empty string (`\"\"`).\n",
       "*   `yield from ...`: This is a feature of Python 3.3+ generators that allows us to yield all values from another generator and continue executing. It's equivalent to writing this in a loop:\n",
       "\n",
       "    ```python\n",
       "for author in {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    yield author\n",
       "```\n",
       "\n",
       "**Why does it work?**\n",
       "\n",
       "When we use `yield from`, Python executes the inner generator until it hits a `StopIteration` exception (which occurs when there are no more values to yield). At that point, control returns to the outer generator.\n",
       "\n",
       "In our example, this means that for every book in `books` with an author, we'll yield one value. If there's an empty dictionary or a dictionary without \"author\" key, it won't affect the execution of the rest of the expression.\n",
       "\n",
       "**Example use case:**\n",
       "\n",
       "Suppose you're building a library management system and want to create a list of authors from your collection of books. You can use this code as follows:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"The Great Gatsby\", \"author\": \"F. Scott Fitzgerald\"},\n",
       "    {\"title\": \"To Kill a Mockingbird\", \"author\": \"Harper Lee\"},\n",
       "    {\"title\": \"Pride and Prejudice\", \"author\": None},  # This one is missing an author\n",
       "]\n",
       "\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: ['F. Scott Fitzgerald', 'Harper Lee']\n",
       "```\n",
       "\n",
       "In this example, the code filters out any books without an \"author\" key and yields only those with authors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat( model= MODEL_LLAMA, messages= messages)\n",
    "\n",
    "result = response.message.content\n",
    "display(Markdown(result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a5a26f-fef5-469e-a80c-4add251f9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Prompts\n",
    "system_prompt = (\n",
    "        \"You are a helpful technical tutor/assistant who answers questions about Python code, \"\n",
    "        \"software engineering, data science, LLMs, and any programming-related questions.\"\n",
    "    )\n",
    "user_prompt = f\"Please give a detailed explanation to the following question: {user_question}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce94529-20a6-4dd2-8a63-ae10a4b8fd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c94c12-6be1-445f-aba2-80b50c8c44d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cc2cf2b-ec21-40b9-82fd-9a44115074a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concise system prompt\n",
    "system_prompt = \"You are a helpful assistant for Python, software engineering, and data science questions.\"\n",
    "\n",
    "# User prompt remains dynamic\n",
    "user_prompt = f\"Answer this question concisely: {user_question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295d17f2-1e1f-4934-97b6-859b4d66237e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Welcome to Saasy AI Chat! Type 'exit' to quit.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Data Science and AI Engineer\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Saasy**: A Data Scientist and an AI Engineer share similar responsibilities but with distinct focus areas:\n",
       "\n",
       "**Data Scientist:**\n",
       "\n",
       "* Extract insights from data using statistical models and machine learning algorithms\n",
       "* Focus on data wrangling, visualization, and analysis\n",
       "* Often works on predictive modeling, recommendation systems, and natural language processing tasks\n",
       "\n",
       "**AI Engineer (also known as Machine Learning Engineer):**\n",
       "\n",
       "* Builds, trains, and deploys AI/ML models in production environments\n",
       "* Focuses on model development, deployment, and maintenance\n",
       "* Works on developing scalable architectures, model serving, and data pipelines"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  help me with the sample code\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Saasy**: I'd be happy to assist you with your Python project. However, I don't see any specific code or problem you're trying to solve. Can you please provide more details about what kind of code you need help with (e.g. data science, web development, etc.) and what issue you're facing?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  i want to load dataset \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Saasy**: To load a dataset in Python, you can use the following libraries:\n",
       "\n",
       "1. **Pandas**: `df = pd.read_csv('dataset.csv')`\n",
       "2. **NumPy**: `data = np.load('dataset.npy')`\n",
       "3. **Scikit-learn**: `from sklearn.datasets import load_dataset; dataset = load_dataset('dataset_name')`\n",
       "\n",
       "Replace `'dataset.csv'`, `'dataset.npy'`, or `'dataset_name'` with the actual path to your dataset file.\n",
       "\n",
       "For example:\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "df = pd.read_csv('data.csv')\n",
       "```\n",
       "This will load a CSV file named `data.csv` into a Pandas DataFrame."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  thanks\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Saasy**: You're welcome! Is there something I can help you with regarding Python or another topic?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  love?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Saasy**: Love is often considered an abstract concept in programming terms, but in the context of data science or machine learning, \"love\" can be interpreted as:\n",
       "\n",
       "1. **Affinity**: Measuring user affinity towards a brand or product using sentiment analysis.\n",
       "2. **Recommendation Systems**: Identifying users who are likely to show interest in a product based on their past behavior and preferences.\n",
       "\n",
       "However, love is often considered a more complex, subjective, and emotional concept that cannot be directly represented as a numerical value or algorithmic output."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Goodbye!** ðŸ‘‹"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Introduction message\n",
    "display(Markdown(\"### Welcome to Saasy AI Chat! Type 'exit' to quit.\\n\\n\"))\n",
    "\n",
    "# Optimized dynamic chat loop\n",
    "while True:\n",
    "    user_question = input(\"You: \")\n",
    "\n",
    "    if user_question.lower() in ['exit', 'quit']:\n",
    "        display(Markdown(\"**Goodbye!** ðŸ‘‹\"))\n",
    "        break\n",
    "\n",
    "    # # Concise system prompt\n",
    "    # system_prompt = \"You are a helpful assistant for Python, software engineering, and data science questions.\"\n",
    "\n",
    "    # # User prompt remains dynamic\n",
    "    # user_prompt = f\"Answer this question concisely: {user_question}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Try with optimized parameters\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_LLAMA,\n",
    "            messages=messages,\n",
    "        )\n",
    "        result = response.message.content\n",
    "        display(Markdown(f\"**Saasy**: {result}\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Error:** {str(e)}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37033c8-5f24-4327-839b-aa986df5ce39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17eae7-8e58-4fa1-b756-54cb524a53ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73257964-4b5b-479e-9015-2eb3b264ac5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f25fe2-7a30-4458-aaa6-77b0230c4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2852c092-5fc1-42ea-a30e-ab80adb1fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa5c091-0e6b-4489-a055-01bf0e9b0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(question, model):\n",
    "    \"\"\"\n",
    "    Query the specified model and return its response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model= MODEL_LLAMA,\n",
    "        messages= messages,\n",
    "    )\n",
    "    result = response.message.content\n",
    "    display(Markdown(result)) \n",
    "\n",
    "def get_answers(question):\n",
    "    \"\"\"\n",
    "    Fetch answers from both GPT-4o-mini and Llama 3.2.\n",
    "    \"\"\"\n",
    "    # gpt_answer = query_model(question, MODEL_GPT)\n",
    "    llama_answer = query_model(question, MODEL_LLAMA)\n",
    "    \n",
    "    return llama_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0310524f-c94b-434c-97bd-12a3195f6b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476a37c206c946359252039527107cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value=' ', description='Question:', layout=Layout(height='100px', width='100%')), Buttâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def interactive_tool():\n",
    "    \"\"\"\n",
    "    interactive tool in Jupyter Notebook to query models.\n",
    "    \"\"\"\n",
    "    # Widgets\n",
    "    question_input = widgets.Textarea(\n",
    "        value= \" \",\n",
    "        description=\"Question:\",\n",
    "        layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_submit(change):\n",
    "        question = question_input.value\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            display(Markdown(f\"#### Question:\\n{question}\"))\n",
    "            gpt_answer, llama_answer = get_answers(question)\n",
    "            # display(Markdown(f\"### GPT-4o-mini Answer:\\n\\n{gpt_answer}\"))\n",
    "            display(Markdown(f\"#### Sassy:\\n\\n{llama_answer}\"))\n",
    "    \n",
    "    submit_button = widgets.Button(description=\"Submit Question\")\n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    display(widgets.VBox([question_input, submit_button, output_area]))\n",
    "\n",
    "interactive_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7b23d-c628-4895-8bc7-645ae7503495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
